v0.2 (February 25, 2015)
========================

Main updates: added the ability to use different final nonlinearities (instead of just the exponential), and pretty printing of optimization results using the ``tableprint`` module

New Features
------------
- Added a ``nonlinearities.py`` module that lets you customize the final model nonlinearity
- Added a ``check_grad`` option to the ``LNLN.fit`` method, for numerically validating analytical gradients
- Added an LN model class that uses a gaussian log-likelihood (mean squared error), *untested*
- Added a function ``print_test_results`` to ``NeuralEncodingModel`` that nicely prints test results in a formatted ASCII table
- Added the ``tableprint`` module as a dependency

API Changes
-----------
- Added the ability to specify custom final nonlinearities in ``LNLN``, using the ``nonlinearities`` module
- Changed how the ``LNLN.rate()`` function works, it now returns the derivative of the final nonlinearity (instead of the log firing rate). This is used internally in ``LNLN.fit()`` to compute the objective and gradient using different final nonlinearities
- The ``test()`` function now expects the parameters to get passed in, instead of using ``self.theta``. The ``metrics`` function in each subclass of ``NeuralEncodingModel`` should accept a dictionary of parameters in addition to the index of the minibatch to test. This was necessary to be able to test the model on held out data during optimization (before the final parameters were saved in ``self.theta``)